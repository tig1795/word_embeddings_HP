{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/iuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen des Datensatzes \"ff_fanfiction.csv\"\n",
    "ff = pd.read_csv(\"/home/iuser/DH/DH_2020_2021/word_embeddings_HP/Datensatz/df_fanfiction.csv\", encoding='utf-8')\n",
    "\n",
    "#Struktur von \"ff\" und erste 10 Einträge\n",
    "#ff.shape #(9811, 5)\n",
    "#ff.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wegen starker Unausgegligenheit, wurde die ursprüngliche Länge des Korpus der Länge vom \"Harry Potter\"-Datensatz \n",
    "#angepasst, also gekürzt - siehe \"Sophia.ipynb\" -.\n",
    "ff_short = ff[:21] \n",
    "ff_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aus \"Word2vec_HP_1_7_ohne Stoppwörter\"\n",
    "stopwords = stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion zur Bereinigung und Tokenisierung\n",
    "\n",
    "def sentence_to_wordlist(raw:str):\n",
    "    text = re.sub('[^A-Za-z_äÄöÖüÜß]',' ', raw).split() # Umlaute werden hinzugefügt\n",
    "    filtered_text = [word for word in text if word not in stopwords]\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion zur Extraktion der Sätze\n",
    "# gibt eine List von tokenisierten Sätzen aus \n",
    "# berücksichtigt auch Bigramme\n",
    "\n",
    "def prepare_text(raw_text):\n",
    "    raw_sentences = tokenizer.tokenize(raw_text.lower())\n",
    "    tokenized_sentences = Parallel(n_jobs=-1)(delayed(sentence_to_wordlist)(raw_sentence) for raw_sentence in raw_sentences)\n",
    "    phrases = Phrases(tokenized_sentences)\n",
    "    bigram = Phraser(phrases)\n",
    "    sentences = list(bigram[tokenized_sentences])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zu verfeinern\n",
    "\n",
    "text1 = ff_short.Text[0] \n",
    "text2 = ff_short.Text[1]\n",
    "text3 = ff_short.Text[2]\n",
    "text4 = ff_short.Text[3]\n",
    "text5 = ff_short.Text[4]\n",
    "text6 = ff_short.Text[5]\n",
    "text7 = ff_short.Text[6]\n",
    "\n",
    "books = text1+text2+text3+text4+text5+text6+text7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = prepare_text(books)\n",
    "\n",
    "# sentences ist eine Liste von tokenisierten Sätzen, zum Beispiel:\n",
    "print(sentences[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ff_short.Text[2] # \n",
    "\n",
    "sentences = prepare_text(text)\n",
    "\n",
    "# sentences ist eine Liste von tokenisierten Sätzen, zum Beispiel:\n",
    "print(sentences[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training von Word2Vec (mit Gensim)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramter setzen\n",
    "workers = 4\n",
    "seed = 42 #just because"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordner anlegen zum Abspeichern von trainierten Modellen\n",
    "if not os.path.exists('trained_models'):\n",
    "    os.makedirs('trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "w2v_ff_short = Word2Vec(sentences=sentences, \n",
    "                   vector_size=300,  # \"size\"-Parameter musste unbenannt werden\n",
    "                   window=7, \n",
    "                   min_count=3,\n",
    "                   workers=workers,\n",
    "                   sg=1,\n",
    "                   seed=seed)\n",
    "\n",
    "# trainiertes Modell speichern\n",
    "w2v_ff_short.save(os.path.join('trained_models', 'w2v_ff_short.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainiertes Modell laden\n",
    "w2v_ff_short = Word2Vec.load(os.path.join('trained_models', 'w2v_ff_short.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exploration des Word2Vec-Modells</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Wörter als Vektoren</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleicht 2 Vektoren (Cosinus Similarität)\n",
    "\n",
    "w2v_ff_short.wv.similarity('', '')  # \"similarity\" Method deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_ff_short.wv.similarity('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wortvektoren als DataFrame \n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=seed)\n",
    "word_vectors = w2v_ff_short.wv.vectors  # deprecated `syn0`\n",
    "word_vectors_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "points = pd.DataFrame(\n",
    "    [\n",
    "        (word, coords[0], coords[1])\n",
    "        for word, coords in [\n",
    "            (word, word_vectors_2d[w2v_ff_short.wv.key_to_index[word]])  # 'model.wv.vocab[\"word\"].index' deprecated\n",
    "            for word in w2v_ff_short.wv.key_to_index\n",
    "        ]\n",
    "    ],\n",
    "    columns=['word', 'x', 'y'])\n",
    "\n",
    "# random 5 Wörter und ihre Koordinaten ausgeben\n",
    "points.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alle Wortvektoren plotten\n",
    "\n",
    "sns.set_context('poster')\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "points.plot.scatter('x', 'y', s=10, figsize=(20, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion, um durch den Plot zu navigieren (ranzoomen)\n",
    "\n",
    "def plot_region(x_bounds, y_bounds, padding=0.005, fontsize=11):\n",
    "    myslice = points[\n",
    "        (x_bounds[0] <= points.x) &\n",
    "        (points.x <= x_bounds[1]) & \n",
    "        (y_bounds[0] <= points.y) &\n",
    "        (points.y <= y_bounds[1])]\n",
    "    \n",
    "    ax = myslice.plot.scatter('x', 'y', s=35, figsize=(9, 5))\n",
    "    for i, point in myslice.iterrows():\n",
    "        ax.text(point.x + padding, point.y + padding, point.word, fontsize=fontsize)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranzoomen\n",
    "x_bounds = (15, 20)       # Bereich x-Achse einstellen\n",
    "y_bounds = (30, 35)       # Bereich y-Achse einstellen\n",
    "\n",
    "#myslice = points[\n",
    " #       (x_bounds[0] <= points.x) &\n",
    "  #      (points.x <= x_bounds[1]) & \n",
    "   #     (y_bounds[0] <= points.y) &\n",
    "    #    (points.y <= y_bounds[1])]\n",
    "\n",
    "len(myslice)\n",
    "\n",
    "plot_region(x_bounds=x_bounds, y_bounds=y_bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nur ausgewählte Wortvektoren plotten\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=seed)\n",
    "\n",
    "words =  ['']\n",
    "\n",
    "vectors = [w2v_ff_short.wv[word] for word in words]  # deprecated `__getitem__`, self.wv.__getitem__() instead\n",
    "plt.figure(figsize=[30,25])\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "Y = tsne.fit_transform(vectors[:200])\n",
    "plt.scatter(Y[:, 0], Y[:, 1])\n",
    "for label, x, y in zip(words, Y[:, 0], Y[:, 1]):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Ähnliche Wörter</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_ff_short.wv.most_similar(positive=[''], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_ff_short.wv.most_similar(positive=[''], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_ff_short.wv.most_similar(positive=['hexe'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion zum Plotten ähnlicher Wörter im Vektorraum\n",
    "\n",
    "def plot_closest_words(model, word, plot_style):\n",
    "    \n",
    "    arr = np.empty((0,300), dtype='f') \n",
    "    closest_words = model.similar_by_word(word)\n",
    "    word_labels = [word]\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    \n",
    "    for wrd_score in closest_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "                \n",
    "    plt.figure(figsize=[12,6])\n",
    "    plt.style.use(plot_style)\n",
    "                    \n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    \n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_closest_words(w2v_ff_short.wv, '', 'seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_closest_words(w2v_ff_short.wv, '', 'seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Verrechnung der Vektoren</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion zur Analogiebildung\n",
    "\n",
    "def analogy(model, word1, word2, word3):\n",
    "    similarities = model.most_similar(positive=[word1, word3], negative=[word2])\n",
    "    word4 = similarities[0][0]\n",
    "    print('{word1} steht in Beziehung zu {word2}, wie {word4} zu {word3}'.format(**locals()))\n",
    "    return word4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy(w2v_ff_short.wv, '', '', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy(w2v_ff_short.wv, '', '', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
